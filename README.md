# WaveMesh-Diff

**3D Mesh Generation using Diffusion Models in Wavelet Domain**

PhÃ¡t sinh 3D mesh tá»« multi-view images sá»­ dá»¥ng diffusion models trÃªn sparse 3D wavelet coefficients.

---

## ğŸ¯ Tá»•ng Quan

WaveMesh-Diff káº¿t há»£p 4 modules chÃ­nh:

1. **Module A - Wavelet Transform**: Chuyá»ƒn 3D SDF â†’ sparse wavelet coefficients
2. **Module B - Sparse U-Net**: Denoising network cho diffusion
3. **Module C - Gaussian Diffusion**: DDPM/DDIM training vÃ  sampling
4. **Module D - Multi-view Encoder**: Encode images tá»« nhiá»u gÃ³c nhÃ¬n

**Æ¯u Ä‘iá»ƒm:**
- âœ… Tiáº¿t kiá»‡m memory (sparse representation)
- âœ… Scalable (cÃ³ thá»ƒ tÄƒng resolution)
- âœ… Conditioning tá»« multi-view images
- âœ… Topology-consistent meshing

---

## ğŸš€ Báº¯t Äáº§u Nhanh

### 1. CÃ i Äáº·t

```bash
# Clone repository
git clone https://github.com/HoangNguyennnnnnn/WaveMeshDf.git
cd WaveMeshDf

# CÃ i dependencies cÆ¡ báº£n
pip install torch torchvision numpy
pip install PyWavelets trimesh matplotlib

# TÃ¹y chá»n: CÃ i Ä‘áº§y Ä‘á»§
pip install transformers huggingface_hub  # Cho DINOv2
pip install pyrender                      # Cho rendering
```

### 2. Test Installation

```bash
# Test táº¥t cáº£ modules
python test_all_modules.py

# Ká»³ vá»ng: 3/4 hoáº·c 4/4 modules PASS
# (Module A cáº§n PyWavelets)
```

### 3. Quick Example

```python
# Example: Sá»­ dá»¥ng cÃ¡c modules
from data import mesh_to_sdf_simple, sdf_to_sparse_wavelet
from models import WaveMeshUNet, GaussianDiffusion, MultiViewEncoder

# Module A: Mesh â†’ Wavelet
sdf = mesh_to_sdf_simple(mesh, resolution=32)
coeffs, coords = sdf_to_sparse_wavelet(sdf)

# Module D: Multi-view encoding
encoder = MultiViewEncoder(feature_dim=768)
conditioning = encoder(images, camera_poses)

# Module B + C: Diffusion
unet = WaveMeshUNet(context_dim=768, use_attention=True)
diffusion = GaussianDiffusion()

# Training
loss = diffusion(x, context=conditioning)
```

**ğŸ“– Xem [QUICKSTART.md](QUICKSTART.md) Ä‘á»ƒ biáº¿t chi tiáº¿t.**

---

## ğŸ“ Cáº¥u TrÃºc Project

```
WaveMesh-Diff/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ wavelet_utils.py          # Module A: Wavelet transform
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unet_sparse.py            # Module B: Sparse U-Net
â”‚   â”œâ”€â”€ diffusion.py              # Module C: Diffusion model
â”‚   â”œâ”€â”€ multiview_encoder.py      # Module D: Multi-view encoder
â”‚   â””â”€â”€ spconv_compat.py          # Sparse conv compatibility layer
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.py          # Download datasets
â”‚   â””â”€â”€ render_multiview.py       # Render multi-view images
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_spconv_compat.py
â”‚   â”œâ”€â”€ test_modules_bc.py
â”‚   â””â”€â”€ test_wavelet_pipeline.py
â”œâ”€â”€ test_all_modules.py           # Comprehensive testing
â”œâ”€â”€ test_module_d.py              # Module D testing
â”œâ”€â”€ visualize_results.py          # Visualization script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ QUICKSTART.md                 # Quick start guide
â”œâ”€â”€ ROADMAP.md                    # Development roadmap
â”œâ”€â”€ ARCHITECTURE.md               # Technical details
â””â”€â”€ TROUBLESHOOTING.md            # Common issues
```

---

## ğŸ—ï¸ Architecture Overview

### Module A: Wavelet Transform 3D

Chuyá»ƒn Ä‘á»•i giá»¯a 3D SDF vÃ  sparse wavelet coefficients.

**API:**
```python
from data import mesh_to_sdf_simple, sdf_to_sparse_wavelet, sparse_wavelet_to_sdf

# Mesh â†’ SDF â†’ Wavelet
sdf = mesh_to_sdf_simple(mesh, resolution=32)
coeffs, coords = sdf_to_sparse_wavelet(sdf, threshold=0.01)

# Reconstruct
sdf_recon = sparse_wavelet_to_sdf(coeffs, coords, shape=(32,32,32))
```

### Module B: Sparse U-Net

3D U-Net vá»›i sparse convolutions, time embedding, vÃ  cross-attention.

**API:**
```python
from models import WaveMeshUNet

model = WaveMeshUNet(
    in_channels=1,
    encoder_channels=[32, 64, 128],
    decoder_channels=[128, 64, 32],
    time_emb_dim=256,
    use_attention=True,
    context_dim=768  # Cho conditioning
)

output = model(x_sparse, timestep, context=conditioning)
```

### Module C: Gaussian Diffusion

DDPM vÃ  DDIM diffusion process.

**API:**
```python
from models import GaussianDiffusion

diffusion = GaussianDiffusion(
    timesteps=1000,
    beta_schedule='linear'
)

# Training
loss = diffusion.compute_loss(x_start)

# Sampling
samples = diffusion.sample(shape=(B, C, H, W, D), method='ddim', steps=50)
```

### Module D: Multi-view Encoder

Encode multi-view images thÃ nh conditioning features.

**API:**
```python
from models import MultiViewEncoder, create_multiview_encoder

# CÃ¡ch 1: Manual
encoder = MultiViewEncoder(
    image_size=224,
    feature_dim=768,
    num_heads=8
)

# CÃ¡ch 2: Preset
encoder = create_multiview_encoder(preset='base')  # 'small', 'base', 'large'

# Usage
images = torch.randn(B, N_views, 3, 224, 224)
poses = torch.randn(B, N_views, 3, 4)
conditioning = encoder(images, poses)  # (B, N_views, 768)
```

**ğŸ“– Xem [ARCHITECTURE.md](ARCHITECTURE.md) Ä‘á»ƒ biáº¿t chi tiáº¿t ká»¹ thuáº­t.**

---

## ğŸ“Š Training

### Chuáº©n Bá»‹ Data

```bash
# Download ModelNet40 (500MB - quick start)
python scripts/download_data.py --dataset modelnet40

# Hoáº·c download ShapeNet (50GB - better quality)
python scripts/download_data.py --dataset shapenet
# Follow instructions Ä‘á»ƒ Ä‘Äƒng kÃ½
```

### Training Pipeline

Xem **[ROADMAP.md](ROADMAP.md)** Ä‘á»ƒ cÃ³:
- Dataset implementation Ä‘áº§y Ä‘á»§
- Training loop vá»›i all 4 modules
- Evaluation metrics
- Improvement suggestions

### Quick Test

```bash
# Overfit test (verify code works)
python train_simple.py --num_samples 10 --num_epochs 50

# Ká»³ vá»ng: Loss tá»« ~0.5 â†’ ~0.01
```

---

## ğŸ§ª Testing

```bash
# Test táº¥t cáº£ modules
python test_all_modules.py

# Test riÃªng Module D
python test_module_d.py

# Test specific modules
python -m pytest tests/ -v
```

**Test Results:**
- âœ… Module B: Sparse U-Net (395K params)
- âœ… Module C: Gaussian Diffusion (DDPM/DDIM)
- âœ… Module D: Multi-view Encoder (with fallback)
- âš ï¸ Module A: Cáº§n cÃ i PyWavelets

---

## ğŸ“ˆ Performance

### Current Status

- **Backend**: Dense fallback mode (chÆ°a cÃ i spconv)
- **Vision**: CNN fallback (chÆ°a cÃ i transformers)
- **Status**: âœ… All modules tested vÃ  hoáº¡t Ä‘á»™ng

### Production Setup

```bash
# Full performance
pip install spconv-cu118          # GPU sparse convolutions
pip install transformers          # Pre-trained DINOv2
huggingface-cli login            # Download DINOv2 weights
```

### Expected Speed

| Setup | Resolution | Time/Epoch | Hardware |
|-------|-----------|-----------|----------|
| CPU Dense | 32Â³ | ~30 min | i7 |
| GPU Dense | 32Â³ | ~5 min | RTX 3080 |
| GPU Sparse | 32Â³ | ~2 min | RTX 3080 + spconv |
| GPU Sparse | 64Â³ | ~8 min | RTX 3080 + spconv |

---

## ğŸ› Troubleshooting

### Common Issues

**"ModuleNotFoundError: No module named 'pywt'"**
```bash
pip install PyWavelets
```

**"transformers not available"**
```bash
pip install transformers huggingface_hub
# Code tá»± Ä‘á»™ng fallback sang CNN
```

**"CUDA out of memory"**
```bash
# Giáº£m batch size hoáº·c resolution
python train.py --batch_size 2 --resolution 16
```

**ğŸ“– Xem [TROUBLESHOOTING.md](TROUBLESHOOTING.md) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.**

---

## ğŸ“š Documentation

- **[README.md](README.md)** - Project overview (file nÃ y)
- **[QUICKSTART.md](QUICKSTART.md)** - Báº¯t Ä‘áº§u trong 30 phÃºt
- **[ROADMAP.md](ROADMAP.md)** - Lá»™ trÃ¬nh training & improvement
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Chi tiáº¿t ká»¹ thuáº­t
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Giáº£i quyáº¿t lá»—i

---

## ğŸ¯ Roadmap

### Hiá»‡n Táº¡i (v0.1)
- âœ… 4 modules hoÃ n chá»‰nh
- âœ… Testing infrastructure
- âœ… Documentation
- âš ï¸ ChÆ°a cÃ³ trained weights

### Tiáº¿p Theo (v0.2)
- [ ] Training scripts hoÃ n chá»‰nh
- [ ] Pre-trained weights
- [ ] Evaluation metrics
- [ ] Demo notebooks

### TÆ°Æ¡ng Lai (v1.0)
- [ ] Multi-GPU training
- [ ] Classifier-free guidance
- [ ] Progressive training
- [ ] Web demo

**ğŸ“– Xem [ROADMAP.md](ROADMAP.md) Ä‘á»ƒ biáº¿t chi tiáº¿t.**

---

## ğŸ“„ License

MIT License

---

## ğŸ™ Acknowledgments

- **Diffusion Models**: DDPM, DDIM papers
- **3D Generation**: Point-E, Shap-E (OpenAI)
- **Vision Encoder**: DINOv2 (Meta)
- **Datasets**: ShapeNet, ModelNet40

---

## ğŸ“ Contact

- **GitHub**: [HoangNguyennnnnnn/WaveMeshDf](https://github.com/HoangNguyennnnnnn/WaveMeshDf)
- **Issues**: [Report bugs](https://github.com/HoangNguyennnnnnn/WaveMeshDf/issues)

---

**Happy 3D Generation! ğŸ¨**
