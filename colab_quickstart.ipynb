{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc82d62",
   "metadata": {},
   "source": [
    "# WaveMesh-Diff - Google Colab Quick Start\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HoangNguyennnnnnn/WaveMeshDf/blob/main/colab_quickstart.ipynb)\n",
    "\n",
    "**3D Mesh Generation using Diffusion Models in Wavelet Domain**\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Quick Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. âœ… Setup WaveMesh-Diff in Google Colab\n",
    "2. ğŸ§ª Test all 4 modules (Wavelet, U-Net, Diffusion, Multi-view)\n",
    "3. ğŸ“Š Visualize sparse wavelet representation\n",
    "4. ğŸ¨ Run quick demos\n",
    "\n",
    "**Estimated time: 10-15 minutes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff926a1",
   "metadata": {},
   "source": [
    "## ğŸš€ Setup\n",
    "\n",
    "### 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/HoangNguyennnnnnn/WaveMeshDf.git\n",
    "%cd WaveMeshDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd7919",
   "metadata": {},
   "source": [
    "### 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f671bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i dependencies cÆ¡ báº£n\n",
    "!pip install -q PyWavelets trimesh matplotlib rtree scipy\n",
    "\n",
    "# PyTorch thÆ°á»ng Ä‘Ã£ cÃ³ sáºµn trong Colab\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i thÃªm scikit-image cho marching cubes\n",
    "!pip install -q scikit-image\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification - check imports work\n",
    "try:\n",
    "    import pywt\n",
    "    import trimesh\n",
    "    import matplotlib\n",
    "    from skimage import measure\n",
    "    print(\"âœ… PyWavelets:\", pywt.__version__)\n",
    "    print(\"âœ… Trimesh:\", trimesh.__version__)\n",
    "    print(\"âœ… Matplotlib:\", matplotlib.__version__)\n",
    "    print(\"âœ… scikit-image: OK\")\n",
    "    print(\"\\nğŸ‰ All core dependencies ready!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Missing dependency: {e}\")\n",
    "    print(\"Run the install cells above to fix this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf64352",
   "metadata": {},
   "source": [
    "### 3. Optional: Install Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i transformers cho DINOv2 (tÃ¹y chá»n - cáº£i thiá»‡n quality)\n",
    "!pip install -q transformers huggingface_hub\n",
    "\n",
    "# Login HuggingFace (cáº§n token tá»« https://huggingface.co/settings/tokens)\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19aa53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Test Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test táº¥t cáº£ modules\n",
    "!python test_all_modules.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1c53e",
   "metadata": {},
   "source": [
    "**Note:** Náº¿u gáº·p lá»—i import, restart runtime vÃ  cháº¡y láº¡i tá»« Ä‘áº§u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860746b",
   "metadata": {},
   "source": [
    "**Ká»³ vá»ng:**\n",
    "```\n",
    "Results: 4/4 modules passed\n",
    "  Module A âœ… PASS\n",
    "  Module B âœ… PASS\n",
    "  Module C âœ… PASS\n",
    "  Module D âœ… PASS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613713d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Quick Demo\n",
    "\n",
    "### Module A: Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.wavelet_utils import mesh_to_sdf_simple, sdf_to_sparse_wavelet, sparse_wavelet_to_sdf\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Táº¡o mesh máº«u\n",
    "mesh = trimesh.creation.box(extents=[1, 1, 1])\n",
    "print(f\"Mesh: {len(mesh.vertices)} vertices, {len(mesh.faces)} faces\")\n",
    "\n",
    "# Chuyá»ƒn sang SDF\n",
    "sdf = mesh_to_sdf_simple(mesh, resolution=32)\n",
    "print(f\"SDF shape: {sdf.shape}\")\n",
    "\n",
    "# Wavelet transform - tráº£ vá» dictionary\n",
    "sparse_data = sdf_to_sparse_wavelet(sdf, threshold=0.01)\n",
    "print(f\"Sparse indices: {sparse_data['indices'].shape}\")\n",
    "print(f\"Sparse features: {sparse_data['features'].shape}\")\n",
    "\n",
    "# Calculate sparsity\n",
    "total_elements = 32 ** 3\n",
    "non_zero = len(sparse_data['features'])\n",
    "sparsity = 100 * (1 - non_zero / total_elements)\n",
    "print(f\"Sparsity: {sparsity:.1f}%\")\n",
    "\n",
    "# Reconstruct\n",
    "sdf_recon = sparse_wavelet_to_sdf(sparse_data)\n",
    "mse = np.mean((sdf - sdf_recon) ** 2)\n",
    "print(f\"Reconstruction MSE: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779886df",
   "metadata": {},
   "source": [
    "### Visualize SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee85529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SDF slice\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].imshow(sdf[16, :, :], cmap='RdBu')\n",
    "axes[0].set_title('Original SDF (slice)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sdf_recon[16, :, :], cmap='RdBu')\n",
    "axes[1].set_title('Reconstructed SDF')\n",
    "axes[1].axis('off')\n",
    "\n",
    "diff = np.abs(sdf - sdf_recon)\n",
    "axes[2].imshow(diff[16, :, :], cmap='hot')\n",
    "axes[2].set_title(f'Error (MSE={mse:.6f})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f179dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Module D: Multi-view Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import create_multiview_encoder\n",
    "import torch\n",
    "\n",
    "# Táº¡o encoder\n",
    "encoder = create_multiview_encoder(preset='small')\n",
    "print(f\"Encoder created: {sum(p.numel() for p in encoder.parameters()):,} params\")\n",
    "\n",
    "# Test vá»›i random data\n",
    "batch_size = 2\n",
    "num_views = 4\n",
    "images = torch.randn(batch_size, num_views, 3, 224, 224)\n",
    "poses = torch.randn(batch_size, num_views, 3, 4)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    conditioning = encoder(images, poses)\n",
    "\n",
    "print(f\"Input images: {images.shape}\")\n",
    "print(f\"Input poses: {poses.shape}\")\n",
    "print(f\"Output conditioning: {conditioning.shape}\")\n",
    "print(\"âœ… Multi-view encoder working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440e715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Module B + C: U-Net + Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WaveMeshUNet, GaussianDiffusion\n",
    "\n",
    "# Táº¡o U-Net\n",
    "unet = WaveMeshUNet(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[16, 32, 64],\n",
    "    decoder_channels=[64, 32, 16],\n",
    "    time_emb_dim=128,\n",
    "    use_attention=True,\n",
    "    context_dim=384  # Match Module D output\n",
    ")\n",
    "print(f\"U-Net: {sum(p.numel() for p in unet.parameters()):,} params\")\n",
    "\n",
    "# Táº¡o Diffusion\n",
    "diffusion = GaussianDiffusion(\n",
    "    timesteps=1000,\n",
    "    beta_schedule='linear'\n",
    ")\n",
    "print(f\"Diffusion: {diffusion.timesteps} timesteps\")\n",
    "print(f\"Beta range: [{diffusion.betas[0]:.6f}, {diffusion.betas[-1]:.6f}]\")\n",
    "print(\"âœ… U-Net + Diffusion ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677194c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Download Data\n",
    "\n",
    "### Option 1: ModelNet40 (Quick - 500MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ModelNet40\n",
    "!python scripts/download_data.py --dataset modelnet40\n",
    "\n",
    "# Check downloaded data\n",
    "!ls -lh data/ModelNet40/ 2>/dev/null || echo \"Data downloading... Check scripts/download_data.py for manual instructions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1bda4",
   "metadata": {},
   "source": [
    "### Option 2: ShapeNet (Manual)\n",
    "\n",
    "Äá»ƒ download ShapeNet:\n",
    "1. ÄÄƒng kÃ½ táº¡i https://shapenet.org/\n",
    "2. Download ShapeNetCore.v2\n",
    "3. Upload lÃªn Google Drive\n",
    "4. Mount Drive vÃ  copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy ShapeNet data (náº¿u Ä‘Ã£ cÃ³ trong Drive)\n",
    "# !cp -r /content/drive/MyDrive/ShapeNetCore.v2 ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140b5e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ Advanced Demo: Real ModelNet40 Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f33937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real mesh from ModelNet40\n",
    "import trimesh\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Find first available chair mesh\n",
    "chair_meshes = glob.glob(\"data/ModelNet40/chair/train/*.off\")\n",
    "if chair_meshes:\n",
    "    mesh_path = chair_meshes[0]\n",
    "    print(f\"ğŸ“¦ Loading: {Path(mesh_path).name}\")\n",
    "    \n",
    "    # Load mesh\n",
    "    mesh = trimesh.load(mesh_path, force='mesh')\n",
    "    print(f\"Mesh: {len(mesh.vertices)} vertices, {len(mesh.faces)} faces\")\n",
    "    \n",
    "    # Convert to SDF\n",
    "    sdf_real = mesh_to_sdf_simple(mesh, resolution=64)\n",
    "    print(f\"SDF shape: {sdf_real.shape}\")\n",
    "    \n",
    "    # Wavelet transform\n",
    "    sparse_real = sdf_to_sparse_wavelet(sdf_real, threshold=0.05)\n",
    "    total = 64 ** 3\n",
    "    non_zero = len(sparse_real['features'])\n",
    "    sparsity_real = 100 * (1 - non_zero / total)\n",
    "    \n",
    "    print(f\"Sparse indices: {sparse_real['indices'].shape}\")\n",
    "    print(f\"Sparsity: {sparsity_real:.1f}%\")\n",
    "    print(f\"Compression: {total / non_zero:.1f}x\")\n",
    "    \n",
    "    # Reconstruct\n",
    "    sdf_real_recon = sparse_wavelet_to_sdf(sparse_real)\n",
    "    mse_real = np.mean((sdf_real - sdf_real_recon) ** 2)\n",
    "    print(f\"Reconstruction MSE: {mse_real:.6f}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No chair meshes found. Run download cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f3053",
   "metadata": {},
   "source": [
    "### Visualize Real Mesh Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64265f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chair_meshes:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'Real Mesh Pipeline: {Path(mesh_path).name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Row 1: Different SDF slices\n",
    "    for i, slice_idx in enumerate([16, 32, 48]):\n",
    "        axes[0, i].imshow(sdf_real[slice_idx, :, :], cmap='RdBu', vmin=-1, vmax=1)\n",
    "        axes[0, i].set_title(f'SDF Slice {slice_idx}/64')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 2: Reconstruction analysis\n",
    "    axes[1, 0].imshow(sdf_real_recon[32, :, :], cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[1, 0].set_title('Reconstructed SDF')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Error map\n",
    "    error_real = np.abs(sdf_real - sdf_real_recon)\n",
    "    axes[1, 1].imshow(error_real[32, :, :], cmap='hot')\n",
    "    axes[1, 1].set_title(f'Error (MSE={mse_real:.6f})')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Sparsity visualization\n",
    "    sparse_viz_real = np.zeros((64, 64))\n",
    "    for idx in sparse_real['indices']:\n",
    "        if idx[2] == 32:\n",
    "            sparse_viz_real[idx[0], idx[1]] += 1\n",
    "    axes[1, 2].imshow(sparse_viz_real, cmap='hot')\n",
    "    axes[1, 2].set_title(f'Sparse Coeffs ({sparsity_real:.1f}% sparse)')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"âœ… Real mesh pipeline complete! Compression: {total / non_zero:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62161c",
   "metadata": {},
   "source": [
    "### Multi-view Rendering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f193219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render multiple views of the mesh\n",
    "# Note: Requires display, may not work in headless Colab\n",
    "if chair_meshes:\n",
    "    try:\n",
    "        # Simple multi-view using matplotlib 3D\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        \n",
    "        # 4 different viewing angles\n",
    "        angles = [\n",
    "            (30, 45),   # Front-right\n",
    "            (30, 135),  # Back-right\n",
    "            (30, 225),  # Back-left\n",
    "            (30, 315),  # Front-left\n",
    "        ]\n",
    "        \n",
    "        for i, (elev, azim) in enumerate(angles):\n",
    "            ax = fig.add_subplot(1, 4, i+1, projection='3d')\n",
    "            \n",
    "            # Create mesh collection\n",
    "            mesh_collection = Poly3DCollection(\n",
    "                mesh.vertices[mesh.faces], \n",
    "                alpha=0.7, \n",
    "                facecolor='cyan', \n",
    "                edgecolor='navy',\n",
    "                linewidths=0.1\n",
    "            )\n",
    "            ax.add_collection3d(mesh_collection)\n",
    "            \n",
    "            # Set limits\n",
    "            scale = mesh.vertices.max()\n",
    "            ax.set_xlim([-scale, scale])\n",
    "            ax.set_ylim([-scale, scale])\n",
    "            ax.set_zlim([-scale, scale])\n",
    "            \n",
    "            # Set view angle\n",
    "            ax.view_init(elev=elev, azim=azim)\n",
    "            ax.set_title(f'View {i+1} ({azim}Â°)')\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"âœ… Multi-view rendering complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Multi-view rendering failed: {e}\")\n",
    "        print(\"This is OK - rendering requires display capabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443e8af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ Visualize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275808c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete pipeline\n",
    "# Note: visualize_results.py cáº§n Ä‘Æ°á»£c táº¡o trÆ°á»›c\n",
    "# Hoáº·c dÃ¹ng code Ä‘Æ¡n giáº£n dÆ°á»›i Ä‘Ã¢y:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from data.wavelet_utils import WaveletTransform3D\n",
    "import numpy as np\n",
    "\n",
    "# Simple visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('WaveMesh-Diff Pipeline Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Input SDF\n",
    "axes[0, 0].imshow(sdf[16, :, :], cmap='RdBu')\n",
    "axes[0, 0].set_title('1. Input SDF (slice)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# 2. Wavelet Coefficients (visualize sparsity)\n",
    "sparse_indices = sparse_data['indices']\n",
    "sparse_viz = np.zeros((32, 32))\n",
    "for idx in sparse_indices:\n",
    "    if idx[2] == 16:  # Same slice\n",
    "        sparse_viz[idx[0], idx[1]] += 1\n",
    "axes[0, 1].imshow(sparse_viz, cmap='hot')\n",
    "axes[0, 1].set_title(f'2. Sparse Wavelet ({sparsity:.1f}% sparse)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# 3. Reconstructed SDF\n",
    "axes[1, 0].imshow(sdf_recon[16, :, :], cmap='RdBu')\n",
    "axes[1, 0].set_title('3. Reconstructed SDF')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# 4. Reconstruction Error\n",
    "error = np.abs(sdf - sdf_recon)\n",
    "axes[1, 1].imshow(error[16, :, :], cmap='Reds')\n",
    "axes[1, 1].set_title(f'4. Error (MSE={mse:.6f})')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"âœ… Pipeline visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309aaba6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‹ï¸ Training Example (Conceptual)\n",
    "\n",
    "âš ï¸ **LÆ°u Ã½:** Training Ä‘áº§y Ä‘á»§ cáº§n nhiá»u thá»i gian vÃ  GPU. Xem `ROADMAP.md` Ä‘á»ƒ cÃ³ code Ä‘áº§y Ä‘á»§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d4e2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ End-to-End Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd745959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline: Mesh â†’ SDF â†’ Wavelet â†’ U-Net â†’ Diffusion\n",
    "print(\"ğŸ”„ End-to-End Pipeline Demo\\n\" + \"=\"*60)\n",
    "\n",
    "# Step 1: Input (use simple box for demo)\n",
    "print(\"Step 1: Create input mesh\")\n",
    "demo_mesh = trimesh.creation.box(extents=[1, 1, 1])\n",
    "print(f\"  âœ… Mesh: {len(demo_mesh.vertices)} vertices\")\n",
    "\n",
    "# Step 2: Convert to SDF\n",
    "print(\"\\nStep 2: Convert to SDF\")\n",
    "demo_sdf = mesh_to_sdf_simple(demo_mesh, resolution=16)  # Small for speed\n",
    "print(f\"  âœ… SDF: {demo_sdf.shape}\")\n",
    "\n",
    "# Step 3: Wavelet transform\n",
    "print(\"\\nStep 3: Sparse wavelet representation\")\n",
    "demo_sparse = sdf_to_sparse_wavelet(demo_sdf, threshold=0.05)\n",
    "print(f\"  âœ… Sparse: {demo_sparse['indices'].shape[0]} coefficients\")\n",
    "\n",
    "# Step 4: Prepare for U-Net (convert to dense for demo)\n",
    "print(\"\\nStep 4: Prepare batch for U-Net\")\n",
    "# In real training, we'd use sparse tensor directly\n",
    "# For demo, we'll use a small dense grid\n",
    "demo_input = torch.randn(1, 1, 16, 16, 16)  # (B, C, D, H, W)\n",
    "print(f\"  âœ… Input: {demo_input.shape}\")\n",
    "\n",
    "# Step 5: U-Net denoising\n",
    "print(\"\\nStep 5: U-Net forward pass\")\n",
    "demo_unet = WaveMeshUNet(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[8, 16],\n",
    "    decoder_channels=[16, 8],\n",
    "    time_emb_dim=64,\n",
    "    use_attention=False\n",
    ")\n",
    "timesteps_demo = torch.tensor([500])  # Middle timestep\n",
    "demo_output = demo_unet(demo_input, timesteps_demo, context=None)\n",
    "print(f\"  âœ… Output: {demo_output.shape}\")\n",
    "\n",
    "# Step 6: Diffusion denoising\n",
    "print(\"\\nStep 6: Diffusion sampling (conceptual)\")\n",
    "demo_diffusion = GaussianDiffusion(timesteps=100, beta_schedule='linear')\n",
    "print(f\"  âœ… Diffusion ready: {demo_diffusion.timesteps} steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Complete pipeline working!\")\n",
    "print(\"\\nğŸ“– For full training loop, see ROADMAP.md\")\n",
    "print(\"   â€¢ Dataset loader for ModelNet40/ShapeNet\")\n",
    "print(\"   â€¢ Training with multi-view conditioning\")\n",
    "print(\"   â€¢ Evaluation metrics (Chamfer distance, F-score)\")\n",
    "print(\"   â€¢ Checkpoint saving/loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d7691",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"âš¡ Performance Benchmarks\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Benchmark 1: Wavelet Transform\n",
    "print(\"\\n1. Wavelet Transform Speed\")\n",
    "resolutions = [16, 32, 64]\n",
    "for res in resolutions:\n",
    "    test_sdf = np.random.randn(res, res, res)\n",
    "    \n",
    "    start = time.time()\n",
    "    test_sparse = sdf_to_sparse_wavelet(test_sdf, threshold=0.01)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    sparsity = 100 * (1 - len(test_sparse['features']) / (res**3))\n",
    "    print(f\"  {res}Â³: {elapsed*1000:.1f}ms ({sparsity:.1f}% sparse)\")\n",
    "\n",
    "# Benchmark 2: U-Net Inference\n",
    "print(\"\\n2. U-Net Inference Speed\")\n",
    "test_unet = WaveMeshUNet(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[16, 32],\n",
    "    decoder_channels=[32, 16],\n",
    "    time_emb_dim=64\n",
    ")\n",
    "test_unet.eval()\n",
    "\n",
    "for res in [8, 16, 32]:\n",
    "    test_input = torch.randn(1, 1, res, res, res)\n",
    "    test_t = torch.tensor([100])\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = test_unet(test_input, test_t)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = test_unet(test_input, test_t)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    params = sum(p.numel() for p in test_unet.parameters())\n",
    "    print(f\"  {res}Â³: {elapsed*1000:.1f}ms ({params:,} params)\")\n",
    "\n",
    "# Benchmark 3: Memory Usage\n",
    "print(\"\\n3. Memory Comparison\")\n",
    "for res in [32, 64]:\n",
    "    dense_mb = (res**3 * 4) / (1024**2)  # float32\n",
    "    \n",
    "    test_sdf = np.random.randn(res, res, res)\n",
    "    test_sparse = sdf_to_sparse_wavelet(test_sdf, threshold=0.01)\n",
    "    sparse_mb = (len(test_sparse['features']) * 4) / (1024**2)\n",
    "    \n",
    "    compression = dense_mb / sparse_mb if sparse_mb > 0 else float('inf')\n",
    "    print(f\"  {res}Â³: Dense={dense_mb:.2f}MB, Sparse={sparse_mb:.2f}MB ({compression:.1f}x)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Benchmarks complete!\")\n",
    "print(\"\\nğŸ’¡ Tips for faster training:\")\n",
    "print(\"  â€¢ Use mixed precision (torch.cuda.amp)\")\n",
    "print(\"  â€¢ Install spconv for GPU sparse ops\")\n",
    "print(\"  â€¢ Reduce resolution during development (32Â³ â†’ 16Â³)\")\n",
    "print(\"  â€¢ Enable gradient checkpointing for large models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (conceptual)\n",
    "# Xem ROADMAP.md Ä‘á»ƒ cÃ³ implementation Ä‘áº§y Ä‘á»§\n",
    "\n",
    "from models import create_multiview_encoder, WaveMeshUNet, GaussianDiffusion\n",
    "import torch\n",
    "\n",
    "# 1. Prepare models\n",
    "encoder = create_multiview_encoder(preset='small')\n",
    "unet = WaveMeshUNet(\n",
    "    in_channels=1,\n",
    "    encoder_channels=[16, 32, 64],\n",
    "    decoder_channels=[64, 32, 16],\n",
    "    time_emb_dim=128,\n",
    "    context_dim=384\n",
    ")\n",
    "diffusion = GaussianDiffusion(timesteps=1000)\n",
    "\n",
    "# 2. Optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': encoder.parameters(), 'lr': 1e-5},\n",
    "    {'params': unet.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "print(\"âœ… Training setup ready!\")\n",
    "print(f\"Encoder params: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "print(f\"U-Net params: {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "print(\"\\nğŸ“– See ROADMAP.md for full training code with:\")\n",
    "print(\"  â€¢ Dataset implementation\")\n",
    "print(\"  â€¢ Training loop\")\n",
    "print(\"  â€¢ Evaluation metrics\")\n",
    "print(\"  â€¢ Checkpointing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c9995",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Next Steps\n",
    "\n",
    "1. **Äá»c Documentation:**\n",
    "   - [README.md](https://github.com/HoangNguyennnnnnn/WaveMeshDf/blob/main/README.md) - Project overview\n",
    "   - [QUICKSTART.md](https://github.com/HoangNguyennnnnnn/WaveMeshDf/blob/main/QUICKSTART.md) - Quick start guide\n",
    "   - [ROADMAP.md](https://github.com/HoangNguyennnnnnn/WaveMeshDf/blob/main/ROADMAP.md) - Training roadmap\n",
    "   - [ARCHITECTURE.md](https://github.com/HoangNguyennnnnnn/WaveMeshDf/blob/main/ARCHITECTURE.md) - Technical details\n",
    "\n",
    "2. **Train Your Own Model:**\n",
    "   - Implement dataset loader (see ROADMAP.md)\n",
    "   - Set up training loop with multi-view conditioning\n",
    "   - Add evaluation metrics (Chamfer distance, F-score)\n",
    "   - Save/load checkpoints\n",
    "\n",
    "3. **Improve Quality:**\n",
    "   - Use DINOv2 pretrained encoder (`pip install transformers`)\n",
    "   - Implement classifier-free guidance (CFG)\n",
    "   - Add EMA for stable training\n",
    "   - Use AdaLN for better conditioning\n",
    "\n",
    "4. **Scale Up:**\n",
    "   - Install spconv for GPU sparse operations\n",
    "   - Use mixed precision training (`torch.cuda.amp`)\n",
    "   - Increase resolution (64Â³ or 128Â³)\n",
    "   - Train on ShapeNet (50GB, 55 categories)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› Troubleshooting\n",
    "\n",
    "### \"ModuleNotFoundError: No module named 'pywt'\"\n",
    "```python\n",
    "!pip install PyWavelets\n",
    "```\n",
    "\n",
    "### \"ModuleNotFoundError: No module named 'rtree'\"\n",
    "```python\n",
    "!pip install rtree\n",
    "```\n",
    "\n",
    "### \"ValueError: too many values to unpack (expected 2)\"\n",
    "Lá»—i nÃ y xáº£y ra khi dÃ¹ng API cÅ©. `sdf_to_sparse_wavelet()` tráº£ vá» **dictionary**, khÃ´ng pháº£i tuple:\n",
    "```python\n",
    "# âŒ Sai:\n",
    "coeffs, coords = sdf_to_sparse_wavelet(sdf)\n",
    "\n",
    "# âœ… ÄÃºng:\n",
    "sparse_data = sdf_to_sparse_wavelet(sdf, threshold=0.01)\n",
    "print(sparse_data['indices'].shape)\n",
    "print(sparse_data['features'].shape)\n",
    "```\n",
    "\n",
    "### \"FileNotFoundError: No such file or directory: 'data/ModelNet40/train'\"\n",
    "ModelNet40 structure Ä‘Ã£ Ä‘Æ°á»£c sá»­a. Má»—i category cÃ³ train/test subfolders riÃªng. Script Ä‘Ã£ Ä‘Æ°á»£c update Ä‘á»ƒ xá»­ lÃ½ Ä‘Ãºng structure nÃ y.\n",
    "\n",
    "### \"CUDA out of memory\"\n",
    "```python\n",
    "# Giáº£m batch size hoáº·c resolution\n",
    "batch_size = 2\n",
    "resolution = 16\n",
    "```\n",
    "\n",
    "### \"transformers not available\"\n",
    "```python\n",
    "!pip install transformers huggingface_hub\n",
    "# Code sáº½ tá»± Ä‘á»™ng fallback sang CNN náº¿u khÃ´ng cÃ³\n",
    "```\n",
    "\n",
    "### \"No module named 'skimage'\"\n",
    "```python\n",
    "!pip install scikit-image\n",
    "```\n",
    "\n",
    "### Rendering fails in Colab\n",
    "Multi-view rendering cÃ³ thá»ƒ fail do headless environment. ÄÃ¢y lÃ  bÃ¬nh thÆ°á»ng - code váº«n hoáº¡t Ä‘á»™ng mÃ  khÃ´ng cáº§n visualization.\n",
    "\n",
    "Xem Ä‘áº§y Ä‘á»§ táº¡i [TROUBLESHOOTING.md](https://github.com/HoangNguyennnnnnn/WaveMeshDf/blob/main/TROUBLESHOOTING.md).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Summary\n",
    "\n",
    "**What you learned:**\n",
    "- âœ… Setup WaveMesh-Diff in Google Colab\n",
    "- âœ… Test all 4 modules (Wavelet, U-Net, Diffusion, Multi-view)\n",
    "- âœ… Convert mesh â†’ SDF â†’ sparse wavelet\n",
    "- âœ… Visualize pipeline and reconstruction quality\n",
    "- âœ… Understand memory savings from sparsity (60-90% compression)\n",
    "- âœ… Run complete end-to-end pipeline\n",
    "- âœ… Benchmark performance\n",
    "\n",
    "**Next steps:**\n",
    "- ğŸ“– Read ROADMAP.md for training implementation\n",
    "- ğŸ‹ï¸ Implement dataset loader for ModelNet40\n",
    "- ğŸš€ Train your first diffusion model\n",
    "- ğŸ¨ Generate 3D meshes from multi-view images!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy 3D Generation! ğŸ¨**\n",
    "\n",
    "Questions? Open an issue: https://github.com/HoangNguyennnnnnn/WaveMeshDf/issues"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
